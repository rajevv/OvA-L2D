{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zpBBMQOa0BAr",
        "646FH3P80BAt",
        "6nmWE69d0BAv",
        "peg_saWC0BAw",
        "WlFx9KFC0BAx",
        "M8gO__a70BAx",
        "vXeah0FNMAVv",
        "tcFLUGqJEQYS"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU60ey810BAf",
        "outputId": "306c0252-794e-4fad-b5ea-23e4ccc81431"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import os\n",
        "import pickle5 as pickle\n",
        "%matplotlib inline\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q6A1i856Z_I",
        "outputId": "e4352e41-ee22-4963-a59b-5afebfb9fb26"
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'import warnings' failed; traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/warnings.py\", line 550, in <module>\n",
            "    _processoptions(sys.warnoptions)\n",
            "  File \"/usr/lib/python3.7/warnings.py\", line 208, in _processoptions\n",
            "    _setoption(arg)\n",
            "  File \"/usr/lib/python3.7/warnings.py\", line 224, in _setoption\n",
            "    import re\n",
            "  File \"/usr/lib/python3.7/re.py\", line 127, in <module>\n",
            "    import functools\n",
            "  File \"/usr/lib/python3.7/functools.py\", line 21, in <module>\n",
            "    from collections import namedtuple\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 857, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 525, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fpxBglb6dkK",
        "outputId": "4b5a48ff-fbe6-4cc1-a617-0255b17b78c5"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHSWajzO0vTV"
      },
      "source": [
        "import numpy.random as rand\n",
        "import codecs\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import numpy.linalg as LA\n",
        "import fasttext\n",
        "import copy\n",
        "import pickle5 as pickle\n",
        "import os\n",
        "import torch\n",
        "\n",
        "'''\n",
        "The data can be downloaded from https://github.com/t-davidson/hate-speech-and-offensive-language. put the labeled_data.csv in this directory and run this script using python3 process_text_data.py\n",
        "'''\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    assert(os.path.exists(file_name+'.pkl'))\n",
        "    with open(file_name + '.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "def save_data(data, file_path):\n",
        "    with open(file_path + '.pkl','wb') as f:\n",
        "        pickle.dump(data,f,pickle.HIGHEST_PROTOCOL)  \n",
        "\n",
        "class preprocess_triage_real_data:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def process_hate_speech_data(self, src_file, dest_file):\n",
        "\n",
        "        with open(src_file, 'r') as f:\n",
        "            f.readline()\n",
        "            dict_tweet = {}\n",
        "            response_list = []\n",
        "            human_annotation_list = []\n",
        "            while True:\n",
        "                line_full = f.readline()\n",
        "                if not line_full:\n",
        "                    save_data({'tweets': dict_tweet, 'y': response_list, 'y_h': human_annotation_list}, dest_file)\n",
        "                    return\n",
        "                else:\n",
        "                    if line_full.isspace():\n",
        "                        print('empty')\n",
        "                    else:\n",
        "                        line = line_full.split(',', 7)\n",
        "                        if len(line) == 7:\n",
        "                            tid = line[0]\n",
        "                            tweet = line[-1]\n",
        "                            dict_tweet[tid] = tweet\n",
        "                            y, y_h = self.get_annotations(line[1:-1])\n",
        "                            response_list.append(y)\n",
        "                            human_annotation_list.append(y_h)\n",
        "\n",
        "    def get_annotations(self, list_of_arg):\n",
        "        human_response = []\n",
        "        for i in [1, 2, 3]:\n",
        "            if int(list_of_arg[i]) > 0:\n",
        "                human_response.extend([i - 1] * int(list_of_arg[i]))\n",
        "        response = int(list_of_arg[-1])\n",
        "        return response, human_response\n",
        "\n",
        "    def dict_to_txt(self, tweet_dict, file_w):\n",
        "        with open(file_w, 'w') as f:\n",
        "            for tweet in tweet_dict.values():\n",
        "                f.write(tweet)\n",
        "\n",
        "    def map_range(self, v, l, h, l_new, h_new):\n",
        "        return float(v - l) * ((h_new - l_new) / float(h - l)) + l_new\n",
        "\n",
        "    def convert_tweet_to_vector(self, file_dict, file_vec, file_tweet):\n",
        "        epsilon = 0.01\n",
        "        data_dict = load_data(file_dict)\n",
        "        data_vec = {}\n",
        "        n_data = len(data_dict['y'])\n",
        "        print(n_data)\n",
        "        data_vec['y'] = np.array(data_dict['y'])\n",
        "        data_vec['c'] = np.zeros((n_data,3))\n",
        "        data_vec['hpred'] = np.zeros(n_data)\n",
        "\n",
        "        for ind, human_pred, response in zip(range(n_data), data_dict['y_h'], data_vec['y']):\n",
        "\n",
        "            h0 = float(np.sum([hpred==0 for hpred in human_pred]))\n",
        "            h1 = float(np.sum([hpred==1 for hpred in human_pred]))\n",
        "            h2 = float(np.sum([hpred==2 for hpred in human_pred]))\n",
        "            assert(h0 + h1 + h2 == len(human_pred))\n",
        "            total_votes = float(len(human_pred))\n",
        "            cc = np.array([float(h0/total_votes),float(h1/total_votes),float(h2/total_votes)])\n",
        "            data_vec['c'][ind] = np.array([float(h0/total_votes),float(h1/total_votes),float(h2/total_votes)])\n",
        "            for i,val in enumerate(data_vec['c'][ind]):\n",
        "                if val<epsilon:\n",
        "                    data_vec['c'][ind][i] = epsilon\n",
        "                    data_vec['c'][ind][np.argmax(data_vec['c'][ind])] -= epsilon\n",
        "\n",
        "            c = torch.distributions.categorical.Categorical(probs = torch.tensor(cc))\n",
        "            human = np.random.choice(len(human_pred))\n",
        "            #human = c.sample().item()\n",
        "            data_vec['hpred'][ind] = human_pred[human]\n",
        "\n",
        "        self.dict_to_txt(data_dict['tweets'],file_tweet)\n",
        "        model = fasttext.train_unsupervised(file_tweet, model='skipgram')\n",
        "        x = []\n",
        "        for tid in data_dict['tweets'].keys():\n",
        "            tweet = data_dict['tweets'][tid].replace('\\n', ' ')\n",
        "            x.append(model.get_sentence_vector(tweet).flatten())\n",
        "        data_vec['x'] = np.array(x)\n",
        "\n",
        "        save_data(data_vec, file_vec)\n",
        "\n",
        "    def truncate_data(self, data_file, data_file_tr):\n",
        "        data = load_data(data_file)\n",
        "        n = data['y'].shape[0]\n",
        "        n_tr = int(n / 4)\n",
        "        print('x', data['x'].shape)\n",
        "        print('y', data['y'].shape)\n",
        "        print('c', data['c'].shape)\n",
        "        data['x'] = data['x'][:n_tr]\n",
        "        data['y'] = data['y'][:n_tr]\n",
        "        data['c'] = data['c'][:n_tr]\n",
        "        data['hpred'] = data['hpred'][:n_tr]\n",
        "        print('x', data['x'].shape)\n",
        "        print('y', data['y'].shape)\n",
        "        print('c', data['c'].shape)\n",
        "        save_data(data, data_file_tr)\n",
        "\n",
        "    def split_data(self, frac, file_data, file_data_split):\n",
        "\n",
        "        data = load_data(file_data)\n",
        "\n",
        "        print('x', data['x'].shape)\n",
        "        print('y', data['y'].shape)\n",
        "        print('c', data['c'].shape)\n",
        "        num_data = data['y'].shape[0]\n",
        "        print(num_data)\n",
        "        num_train = int(frac * num_data)\n",
        "        num_test = int((num_data - num_train)/2)\n",
        "        num_val = num_data - (num_test + num_train)\n",
        "        indices = np.arange(num_data)\n",
        "        random.shuffle(indices)\n",
        "        indices_train = indices[:num_train]\n",
        "        indices_val = indices[num_train:num_train+num_val]\n",
        "        indices_test = indices[num_train+num_val:num_train+num_val+num_test]\n",
        "        data_split = {}\n",
        "        data_split['X'] = data['x'][indices_train]\n",
        "        data_split['Y'] = data['y'][indices_train]\n",
        "        data_split['c'] = data['c'][indices_train]\n",
        "        data_split['hpred'] = data['hpred'][indices_train]\n",
        "\n",
        "        val = {}\n",
        "        val['X'] = data['x'][indices_val]\n",
        "        val['Y'] = data['y'][indices_val]\n",
        "        val['c'] = data['c'][indices_val]\n",
        "        val['hpred'] = data['hpred'][indices_val]\n",
        "        data_split['val'] = val\n",
        "\n",
        "        test = {}\n",
        "        test['X'] = data['x'][indices_test]\n",
        "        test['Y'] = data['y'][indices_test]\n",
        "        test['c'] = data['c'][indices_test]\n",
        "        test['hpred'] = data['hpred'][indices_test]\n",
        "        data_split['test'] = test\n",
        "        data_split['dist_mat'] = np.zeros((num_test, num_train))\n",
        "        save_data(data_split, file_data_split)\n",
        "\n",
        "    def change_format_hatespeech(self, data_file, dest_file):\n",
        "        data = load_data(data_file)\n",
        "\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        x_train = scaler.fit_transform(data['X'])\n",
        "        x_val = scaler.transform(data['val']['X'])\n",
        "        x_test = scaler.transform(data['test']['X'])\n",
        "        data['X'] = x_train\n",
        "        data['val']['X'] = x_val\n",
        "        data['test']['X'] = x_test\n",
        "\n",
        "        c = {'0.0': np.copy(data['c'])}\n",
        "        val_c = {'0.0': np.copy(data['val']['c'])}\n",
        "        test_c = {'0.0': np.copy(data['test']['c'])}\n",
        "        data['c'] = c\n",
        "        data['val']['c'] = val_c\n",
        "        data['test']['c'] = test_c\n",
        "        save_data(data, dest_file)\n",
        "\n",
        "def find_human_loss(file_path):\n",
        "    data = load_data(file_path)\n",
        "    loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "\n",
        "    Y = torch.from_numpy(copy.deepcopy(data['Y'])).long()\n",
        "    hprob = torch.log2(torch.from_numpy(copy.deepcopy(data['c']['0.0'])).float())\n",
        "    hloss = loss_func(hprob, Y)\n",
        "    human_prob, _ = torch.max(hprob, axis=1)\n",
        "\n",
        "    val_Y = torch.from_numpy(copy.deepcopy(data['val']['Y'])).long()\n",
        "    val_hprob = torch.log2(torch.from_numpy(copy.deepcopy(data['val']['c']['0.0'])).float())\n",
        "    val_hloss = loss_func(val_hprob, val_Y)\n",
        "    val_human_prob, _ = torch.max(val_hprob, axis=1)\n",
        "\n",
        "\n",
        "    test_Y = torch.from_numpy(copy.deepcopy(data['test']['Y'])).long()\n",
        "    test_hprob = torch.log2(torch.from_numpy(copy.deepcopy(data['test']['c']['0.0'])).float())\n",
        "    test_hloss = loss_func(test_hprob, test_Y)\n",
        "    test_human_prob, _ = torch.max(test_hprob, axis=1)\n",
        "\n",
        "    data['hloss'] = hloss\n",
        "    data['val']['hloss'] = val_hloss\n",
        "    data['test']['hloss'] = test_hloss\n",
        "\n",
        "    data['hprob'] = human_prob\n",
        "    data['val']['hprob'] = val_human_prob\n",
        "    data['test']['hprob'] = test_human_prob\n",
        "    save_data(data,file_path)\n",
        "\n",
        "def main(path):\n",
        "    src_file = path + 'labeled_data.csv'\n",
        "    obj = preprocess_triage_real_data()\n",
        "    dest_file = path + 'data'\n",
        "    tweet_file = path + 'tweets.txt'\n",
        "    vec_file = path + 'data_vectorized'\n",
        "    vec_full_split_file = path + 'input_full'\n",
        "\n",
        "    obj.process_hate_speech_data(src_file,dest_file)\n",
        "    obj.convert_tweet_to_vector(dest_file,vec_file,tweet_file)\n",
        "    obj.split_data(0.6, vec_file , vec_full_split_file)\n",
        "\n",
        "    dest_file = path + 'hatespeech_data'\n",
        "    obj.change_format_hatespeech(vec_full_split_file, dest_file)\n",
        "    find_human_loss(dest_file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p91fTkHZ1YDU",
        "outputId": "adec30d6-90dd-4ae2-fa8c-73441ef916c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vhh2GEo18-s"
      },
      "source": [
        "path =  \"/content/drive/MyDrive/Colab Notebooks/differentiable-learning-under-triage-main/Hatespeech/\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsbmANhJ3R-M"
      },
      "source": [
        "# main(\"/content/drive/MyDrive/Colab Notebooks/differentiable-learning-under-triage-main/Hatespeech/\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlI_ejEc0BAk"
      },
      "source": [
        "def load_data(file_name):\n",
        "    assert(os.path.exists(file_name+'.pkl'))\n",
        "    with open(file_name + '.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amagDbZn0BAk"
      },
      "source": [
        "def save_data(data, file_path):\n",
        "    with open(file_path + '.pkl','wb') as f:\n",
        "        pickle.dump(data,f,pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def set_seed(seed):\n",
        "\t\trandom.seed(seed)\n",
        "\t\tnp.random.seed(seed)\n",
        "\t\ttorch.manual_seed(seed)\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\ttorch.cuda.manual_seed(seed)\n",
        "\t\t\ttorch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "JKsSewUMatD8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQTC-LXu0BAl"
      },
      "source": [
        "#### The hatespeech data can be found [here](https://github.com/t-davidson/hate-speech-and-offensive-language). Features are extracted from each tweet using [fasttext](https://pypi.org/project/fasttext/). Data is preprocessed in \"preprocess_text_data.py\" and saved in \"hatespeech_data.pkl\". Refer to section 6 of the paper for a detailed description of preprocessing and human predictions modeling. In this notebook we only load the data which we previously generated using the \"preprocess_text_data.py\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXfIdzMg0BAm"
      },
      "source": [
        "constraints = [0.2,0.4,0.6,0.8]\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/differentiable-learning-under-triage-main/Hatespeech/hatespeech_data'\n",
        "model_dir = '/content/drive/MyDrive/Colab Notebooks/differentiable-learning-under-triage-main/Hatespeech/models/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkUvLuZr0BAn"
      },
      "source": [
        "#### The CNN architecture that we used for text classification is that of [Kim 2014](https://arxiv.org/abs/1408.5882)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-AeRD7c0BAo"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, n_filters, filter_sizes, output_dim,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        self.conv_0 = nn.Conv1d(in_channels=1,\n",
        "                                out_channels=n_filters,\n",
        "                                kernel_size=(filter_sizes[0]))\n",
        "\n",
        "        self.conv_1 = nn.Conv1d(in_channels=1,\n",
        "                                out_channels=n_filters,\n",
        "                                kernel_size=(filter_sizes[1]))\n",
        "\n",
        "        self.conv_2 = nn.Conv1d(in_channels=1,\n",
        "                                out_channels=n_filters,\n",
        "                                kernel_size=(filter_sizes[2]))\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = text.unsqueeze(1)\n",
        "\n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(2))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(2))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(2))\n",
        "\n",
        "\n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        cat = self.fc(cat)\n",
        "        cat = self.logsoftmax(cat).squeeze()\n",
        "        return cat"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCr-rXT20BAp"
      },
      "source": [
        "#### The CNN architecture for the [surrogate-based triage](https://arxiv.org/abs/2006.01862)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJcGwA9i0BAq"
      },
      "source": [
        "class CNN_rej(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "                        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = fs) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "                \n",
        "        self.convs_rej = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = fs) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc_rej = nn.Linear(len(filter_sizes) * n_filters, 1)\n",
        "        \n",
        "        self.dropout_rej = nn.Dropout(dropout)\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, embedded):\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(2) for conv in self.convs]\n",
        "            \n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        \n",
        "        embedded_rej = embedded.unsqueeze(1)\n",
        "                \n",
        "        conved_rej = [F.relu(conv(embedded)).squeeze(2) for conv in self.convs_rej]\n",
        "                            \n",
        "        pooled_rej = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "                \n",
        "        cat_rej = self.dropout_rej(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        out_rej = self.fc_rej(cat_rej)\n",
        "        \n",
        "        out = self.fc(cat)\n",
        "        out =  torch.cat((out, out_rej), 1)\n",
        "\n",
        "        out = self.softmax(out)\n",
        "        return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ddaz8kMBbR"
      },
      "source": [
        "class CNN_rej_ova(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "                        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = fs) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "                \n",
        "        self.convs_rej = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = fs) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc_rej = nn.Linear(len(filter_sizes) * n_filters, 1)\n",
        "        \n",
        "        self.dropout_rej = nn.Dropout(dropout)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, embedded):\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(2) for conv in self.convs]\n",
        "            \n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        \n",
        "        embedded_rej = embedded.unsqueeze(1)\n",
        "                \n",
        "        conved_rej = [F.relu(conv(embedded)).squeeze(2) for conv in self.convs_rej]\n",
        "                            \n",
        "        pooled_rej = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "                \n",
        "        cat_rej = self.dropout_rej(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        out_rej = self.fc_rej(cat_rej)\n",
        "        \n",
        "        out = self.fc(cat)\n",
        "        out =  torch.cat((out, out_rej), 1)\n",
        "\n",
        "        conf = self.sigmoid(out)\n",
        "        return out, conf"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYhcQTu10BAq"
      },
      "source": [
        "# Our Method :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpBBMQOa0BAr"
      },
      "source": [
        "#### According to the optimal triage policy, the machine only is trained on the points on which the difference between the machine loss and human loss is less than $t_{P,b,m}$ (refer to equation 3 in the paper). This is equivalent to sorting the samples based on the difference of machine loss and human loss and then giving to machine the first $\\max(\\lceil (1-b) \\, |D| \\rceil, p)$ samples where $p$ is the number of samples with $\\ell(m_{\\theta_{t-1}}(x_{[i]}), y_{[i]}) - \\ell(h_{[i]}, y_{[i]}) < 0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDn8VUUS0BAr"
      },
      "source": [
        "def find_machine_samples(machine_loss, hloss,constraint):\n",
        "    \n",
        "    diff = machine_loss - hloss\n",
        "    argsorted_diff = torch.clone(torch.argsort(diff))\n",
        "    num_outsource = int(constraint * machine_loss.shape[0])\n",
        "    index = -num_outsource\n",
        "\n",
        "    while (index < -1 and diff[argsorted_diff[index]] <= 0):\n",
        "        index += 1\n",
        "\n",
        "    machine_list = argsorted_diff[:index]\n",
        "\n",
        "    return machine_list"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPxGjZPJ0BAr"
      },
      "source": [
        "#### Here we train the machine model on its points. In each iteration first, the machine points are found and then the machine model is trained on them. We use early stopping with a patience of 10 epochs based on the performance on the validation set to determine when to stop training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4EWMqlm0BAr"
      },
      "source": [
        "def train_triage(seed, data_path,constraint):  \n",
        "    machine_type = 'Differentiable'\n",
        "    if seed != '':\n",
        "      mct = machine_type + '_seed_' + str(seed) + '_'\n",
        "    else:\n",
        "      mct = machine_type + '_'\n",
        "    print('training machine model using constraint:',constraint,' and machine model: ',machine_type)\n",
        "    data = load_data(data_path)\n",
        "    X = torch.from_numpy(data['X']).float().to(device)\n",
        "    Y = torch.from_numpy(data['Y']).long().to(device)\n",
        "    hloss = data['hloss'].to(device)\n",
        "    \n",
        "    val_X = torch.from_numpy(data['val']['X']).float().to(device)\n",
        "    val_Y = torch.from_numpy(data['val']['Y']).long().to(device)\n",
        "    val_hloss = data['val']['hloss'].to(device)\n",
        "    \n",
        "    batch_size = 64\n",
        "    num_batches = int(X.shape[0] / batch_size)\n",
        "    \n",
        "    num_epochs = 30\n",
        "        \n",
        "    mnet = CNN(vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "               output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "    if constraint == 0.2:\n",
        "        lr = 0.005\n",
        "    else:\n",
        "        lr = 0.0002\n",
        "\n",
        "    optimizer = torch.optim.Adam(mnet.parameters(),lr = lr)\n",
        "    loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = 1000\n",
        "    eps = 1e-4\n",
        "    max_patience = 10\n",
        "    patience = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('----- epoch:',epoch, '-----')\n",
        "        train_loss = 0\n",
        "        with torch.no_grad():\n",
        "            mprim = copy.deepcopy(mnet)\n",
        "        machine_loss = []\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X[i * batch_size: (i + 1) * batch_size]\n",
        "            Y_batch = Y[i * batch_size: (i + 1) * batch_size]\n",
        "            hloss_batch = hloss[i * batch_size: (i + 1) * batch_size]\n",
        "            machine_scores_batch = mprim(X_batch)\n",
        "            machine_loss_batch = loss_func(machine_scores_batch,Y_batch)\n",
        "            machine_loss.extend(machine_loss_batch.detach())\n",
        "            machine_indices = find_machine_samples(machine_loss_batch, hloss_batch, constraint)\n",
        "\n",
        "            X_machine = X_batch[machine_indices]\n",
        "            Y_machine = Y_batch[machine_indices]\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_func(mnet(X_machine),Y_machine)\n",
        "            loss.sum().backward()\n",
        "            optimizer.step()\n",
        "            train_loss += float(loss.mean())\n",
        "\n",
        "        train_losses.append(train_loss / num_batches)\n",
        "        print('machine_loss:', train_loss/num_batches)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            val_machine_scores = mprim(val_X)\n",
        "            val_machine_loss = loss_func(val_machine_scores,val_Y)\n",
        "            \n",
        "            val_machine_indices = find_machine_samples(val_machine_loss,val_hloss,constraint)\n",
        "            val_loss = float(loss_func(mnet(val_X[val_machine_indices]),val_Y[val_machine_indices]).mean())\n",
        "            print('val_loss:',val_loss) \n",
        "            \n",
        "            if val_loss + eps < best_val_loss:\n",
        "                torch.save(mnet.state_dict(), model_dir + 'm_' + mct + str(constraint))\n",
        "                best_val_loss = val_loss\n",
        "                print('updated the model')\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "            val_losses.append(val_loss)\n",
        "        \n",
        "        if patience > max_patience:\n",
        "            print('no progress for 10 epochs... stopping training')\n",
        "            break\n",
        "\n",
        "        print('\\n')\n",
        "            \n",
        "        if epoch%5==0:\n",
        "            fig, ax = plt.subplots()\n",
        "            plt.title('b = ' + str(constraint) + ' epoch : ' + str(epoch),fontsize=22)\n",
        "            plt.xlim(-5.5,1.3)\n",
        "            plt.ylim([-24,8])\n",
        "            plt.xticks([-5,-2,1],[r'$2^{-5}$',r'$2^{-2}$',r'$2^0$'])\n",
        "            plt.yticks([-20,-10,1],[r'$2^{-20}$',r'$2^{-10}$',r'$2^{0}$'])\n",
        "            plt.xlabel(r'Human Loss',fontsize=22)\n",
        "            plt.ylabel(r'Machine Loss',fontsize=22)\n",
        "            line = np.linspace(-6.2,1.3,100)\n",
        "            plt.plot(line,line,'--')\n",
        "            machine_loss = torch.tensor(machine_loss, device = 'cpu')\n",
        "            plt.scatter(np.log2(hloss[:len(machine_loss)].cpu().data.numpy()),np.log2(np.array(machine_loss)),alpha=0.3)  \n",
        "            plt.show()\n",
        "        \n",
        "    \n",
        "        \n",
        "    plt.plot(range(len(train_losses)),train_losses,marker='o',label='train')\n",
        "    plt.plot(range(len(val_losses)),val_losses,marker='o',label='validation')\n",
        "    plt.legend()\n",
        "    plt.title(machine_type + ' b = ' + str(constraint),fontsize=22)\n",
        "    plt.xlabel(r'Time Step t',fontsize=22)\n",
        "    plt.ylabel(r'Machine Loss',fontsize=20)\n",
        "    plt.show()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThDfeVp10BAs",
        "scrolled": true
      },
      "source": [
        "for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "  print(\"seed: \", seed)\n",
        "  if seed != '':\n",
        "    set_seed(seed)\n",
        "  for constraint in constraints:\n",
        "      train_triage(seed, data_path,constraint)\n",
        "      "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "646FH3P80BAt"
      },
      "source": [
        "#### We train the additional model g to approximate difference of machine loss and human loss for unseen samples. The same CNN and the same training procedure is used for training g. Please refer to "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq45aSuy0BAt"
      },
      "source": [
        "def train_g(seed, data_path, machine_type,constraint):\n",
        "    if seed != '':\n",
        "      mct = machine_type + '_seed_' + str(seed) + '_'\n",
        "    else:\n",
        "      mct = machine_type + '_'\n",
        "    print('started training g using the constraint: ',constraint,' Using machine model: ',machine_type)\n",
        "    data = load_data(data_path)\n",
        "    X = torch.from_numpy(copy.deepcopy(data['X'])).float().to(device)\n",
        "    Y = torch.from_numpy(copy.deepcopy(data['Y'])).long().to(device)\n",
        "    hloss = data['hloss'].to(device)\n",
        "        \n",
        "    mnet = CNN(vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "               output_dim=3, dropout=0.5).to(device)\n",
        "    \n",
        "    if machine_type == 'full':\n",
        "        mnet.load_state_dict(torch.load(model_dir + 'm_full'))\n",
        "        \n",
        "    else:\n",
        "        mnet.load_state_dict(torch.load(model_dir + 'm_' + mct + str(constraint)))\n",
        "        \n",
        "    mnet.eval()\n",
        "    \n",
        "    val_X = torch.from_numpy(data['val']['X']).float().to(device)\n",
        "    val_Y = torch.from_numpy(data['val']['Y']).long().to(device)\n",
        "    val_hloss = data['val']['hloss'].to(device)\n",
        "    \n",
        "    batch_size = 64\n",
        "    num_batches = int(X.shape[0] / batch_size)\n",
        "    \n",
        "    num_epochs = 50\n",
        "    \n",
        "    gnet = CNN(vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "               output_dim=2, dropout=0.5).to(device)\n",
        "    \n",
        "    g_optimizer = torch.optim.Adam(gnet.parameters(),lr=0.008)\n",
        "    loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "    \n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = 1000\n",
        "    max_patience = 10\n",
        "    patience = 0\n",
        "    eps = 1e-4\n",
        "    for epoch in range(num_epochs):\n",
        "        print('----- epoch:',epoch, '-----')\n",
        "        g_train_loss = 0\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X[i * batch_size: (i + 1) * batch_size]\n",
        "            Y_batch = Y[i * batch_size: (i + 1) * batch_size]\n",
        "            hloss_batch = hloss[i * batch_size: (i + 1) * batch_size]\n",
        "            machine_loss_batch = loss_func(mnet(X_batch),Y_batch)\n",
        "            machine_indices = find_machine_samples(machine_loss_batch, hloss_batch, constraint)\n",
        "            g_labels = torch.tensor([0 if j in machine_indices else 1 for j in range(hloss_batch.shape[0])]).to(device)\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss = loss_func(gnet(X_batch),g_labels)\n",
        "            g_loss.sum().backward()\n",
        "            g_optimizer.step()\n",
        "            g_train_loss += float(g_loss.mean())\n",
        "        train_losses.append(g_train_loss/num_batches)\n",
        "        print('g_loss:',g_train_loss/num_batches) \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_machine_loss = loss_func(mnet(val_X),val_Y)\n",
        "            val_machine_indices = find_machine_samples(val_machine_loss,val_hloss,constraint)\n",
        "            val_glabels = torch.tensor([0 if j in val_machine_indices else 1 for j in range(val_X.shape[0])]).to(device)\n",
        "            val_loss = loss_func(gnet(val_X),val_glabels)\n",
        "            val_gloss = float(val_loss.mean())\n",
        "            val_losses.append(val_gloss)\n",
        "            print('val_g_loss:',float(val_gloss))\n",
        "            \n",
        "            if val_gloss + eps < best_val_loss:\n",
        "                torch.save(gnet.state_dict(), model_dir + 'g_' + mct + str(constraint))\n",
        "                best_val_loss = val_gloss\n",
        "                print('updated the model')\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "            \n",
        "        if patience > max_patience:\n",
        "            print('no progress for 10 epochs... stopping training')\n",
        "            break\n",
        "\n",
        "                \n",
        "        print('\\n')\n",
        "        \n",
        "    plt.plot(range(len(train_losses)),train_losses,marker = 'o',label='train')\n",
        "    plt.plot(range(len(val_losses)),val_losses,marker = 'o',label='validation')\n",
        "    plt.title('train and validation curve of g using b = ' + str(constraint),fontsize=22)\n",
        "    plt.xlabel('Epoch',fontsize=22)\n",
        "    plt.ylabel(r'g Loss',fontsize=20)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgpS8UiW0BAt",
        "scrolled": true
      },
      "source": [
        "for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "  if seed != '':\n",
        "    set_seed(seed)\n",
        "  for constraint in constraints:\n",
        "      train_g(seed, data_path,'Differentiable',constraint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqM5p8HK0BAt"
      },
      "source": [
        "def get_test_assignments_us(data_path,constraints):\n",
        "    l = []\n",
        "    for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "      machine_type = 'Differentiable'\n",
        "      if seed != '':\n",
        "        mct = machine_type + '_seed_' + str(seed) + '_'\n",
        "      else:\n",
        "        mct = machine_type + '_'\n",
        "      loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "      data = load_data(data_path)\n",
        "      test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "      test_Y = data['test']['Y']\n",
        "      hlabel = data['test']['hpred']\n",
        "      \n",
        "      losses = []\n",
        "      for constraint in constraints:\n",
        "          loss = np.zeros(test_X.shape[0])\n",
        "          num_machine = int((1.0 - constraint) * test_X.shape[0])\n",
        "          mnet = CNN(vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "          mnet.load_state_dict(torch.load(model_dir + 'm_' + mct + str(constraint)))\n",
        "          mnet.eval()\n",
        "          \n",
        "          mlabel = torch.argmax(mnet(test_X),dim=1).cpu().data.numpy()\n",
        "          \n",
        "          gnet = CNN(vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "                output_dim=2, dropout=0.5).to(device)\n",
        "\n",
        "          gnet.load_state_dict(torch.load(model_dir + 'g_' + mct + str(constraint)))\n",
        "          gnet.eval()\n",
        "          \n",
        "          gprediction = torch.exp(gnet(test_X).detach()[:,1])\n",
        "          to_machine = torch.argsort(gprediction)[:num_machine].cpu().data.numpy()\n",
        "          to_human = np.array([i for i in range(test_X.shape[0]) if i not in to_machine])\n",
        "          \n",
        "          mloss = np.not_equal(mlabel,test_Y)\n",
        "          hloss = np.not_equal(hlabel,test_Y)\n",
        "          \n",
        "          loss[to_machine] = mloss[to_machine]\n",
        "          loss[to_human] = hloss[to_human]\n",
        "\n",
        "          losses.append(np.mean(loss))\n",
        "          del mnet\n",
        "          del gnet\n",
        "      l.append(losses)\n",
        "      \n",
        "    if machine_type not in data.keys():\n",
        "        data[machine_type] = {}\n",
        "    l = np.array(l)\n",
        "    print(\"l shape: \", l.shape)\n",
        "    data[machine_type]['agg_loss'] = l\n",
        "    save_data(data,data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B7wjKW30BAu"
      },
      "source": [
        "get_test_assignments_us(data_path,constraints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqVe1O7C0BAu"
      },
      "source": [
        "# def boxplot():\n",
        "#     constraints = [0.4, 0.6, 0.8, 1.0]\n",
        "#     res = load_data('results/Differentiable')\n",
        "#     for constraint in constraints:\n",
        "#         fig, ax = plt.subplots()\n",
        "\n",
        "#         gpred = res[constraint]['gprediction'].cpu()\n",
        "#         mloss = res[constraint]['mcrossloss'].cpu()\n",
        "#         hloss = res[constraint]['hcrossloss'].cpu()\n",
        "#         to_machine = res[constraint]['to_machine']\n",
        "#         to_human = res[constraint]['to_human']\n",
        "#         threshold_map = {0.0: 0.0, 0.2: 0.257, 0.4: 0.257, 0.6: 0.211, 0.8: 0.211, 1.0: 0.373}\n",
        "#         below = [idx for idx in range(gpred.shape[0]) if idx in to_machine]\n",
        "#         above = [idx for idx in range(gpred.shape[0]) if idx in to_human]\n",
        "#         below_data = [np.log2(mloss[idx]) - np.log2(hloss[idx]) for idx in below]\n",
        "#         above_data = [np.log2(mloss[idx]) - np.log2(hloss[idx]) for idx in above]\n",
        "        \n",
        "#         medianprops = dict(linestyle='-.', linewidth=2.5, color='black')\n",
        "\n",
        "#         ax1 = ax.boxplot(np.array(below_data), positions=[0],  widths=0.35,sym='',whis=0,medianprops=medianprops,#usermedians=[np.mean(below_data)],\n",
        "#                          patch_artist=True, boxprops=dict(facecolor=\"C0\"))\n",
        "#         ax2 = ax.boxplot(np.array(above_data), positions=[1],  widths=0.35,sym='',whis=0,medianprops = medianprops,#usermedians=[np.mean(above_data)],\n",
        "#                          patch_artist=True, boxprops=dict(facecolor=\"C1\"))\n",
        "#         plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC3iPjwO0BAu"
      },
      "source": [
        "# boxplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEw91IcZ0BAu"
      },
      "source": [
        "# Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nmWE69d0BAv"
      },
      "source": [
        "## [Confidence Based Triage](https://arxiv.org/abs/2004.13102)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHc1msX50BAv"
      },
      "source": [
        "def train_confidence(seed, data_path,constraint):\n",
        "    machine_type = 'confidence'\n",
        "    if seed != '':\n",
        "      mct = machine_type + '_seed_' + str(seed) + '_'\n",
        "    else:\n",
        "      mct = machine_type + '_'\n",
        "    print('-----training machine model using constraint:',constraint,' and machine model: ',machine_type)\n",
        "    data = load_data(data_path)\n",
        "    X = torch.from_numpy(data['X']).float().to(device)\n",
        "    Y = torch.from_numpy(data['Y']).long().to(device)\n",
        "    hconf = torch.mean(data['hprob']) + torch.zeros(X.shape[0]).to(device)\n",
        "    \n",
        "    val_X = torch.from_numpy(data['val']['X']).float().to(device)\n",
        "    val_Y = torch.from_numpy(data['val']['Y']).long().to(device)\n",
        "    val_hconf = torch.mean(data['hprob']) + torch.zeros(val_X.shape[0]).to(device)\n",
        "    \n",
        "    batch_size = 64\n",
        "    num_batches = int(X.shape[0] / batch_size)\n",
        "    \n",
        "    num_epochs = 30\n",
        "        \n",
        "    mnet = CNN(vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "               output_dim=3, dropout=0.5).to(device)\n",
        "    \n",
        "        \n",
        "    optimizer = torch.optim.Adam(mnet.parameters(),lr = 0.0065)\n",
        "    loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = 1000\n",
        "    max_patience = 10\n",
        "    patience = 0\n",
        "    eps = 1e-4\n",
        "    for epoch in range(num_epochs):\n",
        "        print('----- epoch:',epoch, '-----')\n",
        "        train_loss = 0\n",
        "        with torch.no_grad():\n",
        "            mprim = copy.deepcopy(mnet)\n",
        "        machine_loss = []\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X[i * batch_size: (i + 1) * batch_size]\n",
        "            Y_batch = Y[i * batch_size: (i + 1) * batch_size]\n",
        "            hconf_batch = hconf[i * batch_size: (i + 1) * batch_size]\n",
        "            machine_scores_batch = mprim(X_batch)\n",
        "            machine_conf_batch, _ = torch.max(machine_scores_batch,axis = 1)   \n",
        "            machine_indices = find_machine_samples(hconf_batch,machine_conf_batch,constraint)\n",
        "                \n",
        "            X_machine = X_batch[machine_indices]\n",
        "            Y_machine = Y_batch[machine_indices]\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_func(mnet(X_machine),Y_machine)\n",
        "            loss.sum().backward()\n",
        "            optimizer.step()\n",
        "            train_loss += float(loss.mean())\n",
        "\n",
        "        train_losses.append(train_loss / num_batches)\n",
        "        print('machine_loss:', train_loss/num_batches)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            val_machine_scores = mprim(val_X)\n",
        "            val_machine_conf,_ = torch.max(val_machine_scores,axis=1)\n",
        "\n",
        "            val_machine_indices = find_machine_samples(val_hconf,val_machine_conf,constraint)\n",
        "\n",
        "            val_loss = float(loss_func(mnet(val_X[val_machine_indices]),val_Y[val_machine_indices]).mean())\n",
        "            \n",
        "            val_losses.append(val_loss)\n",
        "            print('val_loss:',val_loss) \n",
        "            \n",
        "            if val_loss + eps <best_val_loss:\n",
        "                torch.save(mnet.state_dict(), model_dir + 'm_' + mct + str(constraint))\n",
        "                best_val_loss = val_loss\n",
        "                print('updated the model')\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "                \n",
        "        if patience > max_patience:\n",
        "            print('no progress for 10 epochs... stopping training')\n",
        "            break\n",
        "      \n",
        "        print('\\n')\n",
        "            \n",
        "    # plt.plot(range(len(train_losses)),train_losses,marker='o',label = 'train')\n",
        "    # plt.plot(range(len(val_losses)),val_losses,marker='o',label = 'validation')\n",
        "    # plt.legend()\n",
        "    # plt.title(machine_type + ' b = ' + str(constraint),fontsize=22)\n",
        "    # plt.xlabel(r'Time Step t',fontsize=22)\n",
        "    # plt.ylabel(r'Machine Loss',fontsize=20)\n",
        "    # plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLmYjRR0BAv",
        "scrolled": true
      },
      "source": [
        "for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "  if seed != '':\n",
        "    set_seed(seed)\n",
        "  for constraint in constraints:\n",
        "    train_confidence(seed, data_path,constraint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCZL-J_J0BAv"
      },
      "source": [
        "def get_test_assignments_confidence(data_path,constraints):\n",
        "    machine_type = 'confidence'\n",
        "    l = []\n",
        "    for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "      if seed != '':\n",
        "        mct = machine_type + '_seed_' + str(seed) + '_'\n",
        "      else:\n",
        "        mct = machine_type + '_'\n",
        "\n",
        "      loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "      data = load_data(data_path)\n",
        "      test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "      test_Y = data['test']['Y']\n",
        "      hlabel = data['test']['hpred']\n",
        "      hconf = (torch.mean(data['hprob']) + torch.zeros(test_X.shape[0])).to(device)\n",
        "      \n",
        "      losses = []\n",
        "      for constraint in constraints:\n",
        "          mnet = CNN(vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "          mnet.load_state_dict(torch.load(model_dir + 'm_' + mct + str(constraint)))\n",
        "          mnet.eval()\n",
        "          mlabel = torch.argmax(mnet(test_X),dim=1).cpu().data.numpy()\n",
        "          loss = np.zeros(test_X.shape[0])\n",
        "          mconf,_ = torch.max(mnet(test_X),axis = 1)\n",
        "          to_machine = find_machine_samples(hconf,mconf,constraint).cpu().data.numpy()\n",
        "\n",
        "          to_human = np.array([i for i in range(test_X.shape[0]) if i not in to_machine])\n",
        "\n",
        "          mloss = np.not_equal(mlabel,test_Y)\n",
        "          hloss = np.not_equal(hlabel,test_Y)\n",
        "\n",
        "          loss[to_machine] = mloss[to_machine]\n",
        "          loss[to_human] =  hloss[to_human]\n",
        "\n",
        "          losses.append(np.mean(loss))\n",
        "          del mnet\n",
        "          \n",
        "      l.append(losses)\n",
        "    \n",
        "    if machine_type not in data.keys():\n",
        "        data[machine_type] = {}\n",
        "\n",
        "    l = np.array(l)\n",
        "    print('l shape: ', l.shape)\n",
        "    data[machine_type]['agg_loss'] = l\n",
        "    \n",
        "    save_data(data,data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r51x0XPF0BAw"
      },
      "source": [
        "get_test_assignments_confidence(data_path,constraints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peg_saWC0BAw"
      },
      "source": [
        "## Full Automation Triage\n",
        "The machine model is trained under full automation and then an additional model g is trained based on the difference of machine and human loss and used to outsource samples at test time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv5_TGp60BAw"
      },
      "source": [
        "def train_full(seed, data_path, machine_type):\n",
        "    print('-----training machine model: ',machine_type)\n",
        "    if seed != '':\n",
        "      mct = machine_type + '_seed_' + str(seed)\n",
        "    else:\n",
        "      mct = machine_type \n",
        "    data = load_data(data_path)\n",
        "    X = torch.from_numpy(data['X']).float().to(device)\n",
        "    Y = torch.from_numpy(data['Y']).long().to(device)\n",
        "    hloss = data['hloss'].to(device)\n",
        "    hconf = torch.mean(data['hprob']) + torch.zeros(X.shape[0]).to(device)\n",
        "    \n",
        "    val_X = torch.from_numpy(data['val']['X']).float().to(device)\n",
        "    val_Y = torch.from_numpy(data['val']['Y']).long().to(device)\n",
        "    val_hloss = data['val']['hloss'].to(device)\n",
        "    val_hconf = torch.mean(data['hprob']) + torch.zeros(val_X.shape[0]).to(device)\n",
        "    \n",
        "    batch_size = 64\n",
        "    num_batches = int(X.shape[0] / batch_size)\n",
        "    val_num_batches = int(val_X.shape[0]/batch_size)\n",
        "    \n",
        "    num_epochs = 30\n",
        "        \n",
        "    mnet = CNN(vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "               output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(mnet.parameters(),lr=0.005)\n",
        "    loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = 1000\n",
        "    max_patience = 10\n",
        "    patience = 0\n",
        "    eps = 1e-4\n",
        "    for epoch in range(num_epochs):\n",
        "        print('----- epoch:',epoch, '-----')\n",
        "        train_loss = 0\n",
        "        machine_loss = []\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X[i * batch_size: (i + 1) * batch_size]\n",
        "            Y_batch = Y[i * batch_size: (i + 1) * batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_func(mnet(X_batch),Y_batch)\n",
        "            loss.sum().backward()\n",
        "            optimizer.step()\n",
        "            train_loss += float(loss.mean())\n",
        "\n",
        "        train_losses.append(train_loss / num_batches)\n",
        "        print('machine_loss:', train_loss/num_batches)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_loss = float(loss_func(mnet(val_X),val_Y).mean())\n",
        "            print('val_loss:',val_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            if val_loss + eps < best_val_loss:\n",
        "                torch.save(mnet.state_dict(), model_dir + 'm_' + mct)\n",
        "                best_val_loss = val_loss\n",
        "                print('updated the model')\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "                \n",
        "        if patience > max_patience:\n",
        "            print('no progress for 10 epochs... stopping training')\n",
        "            break\n",
        "\n",
        "        print('\\n')\n",
        "    \n",
        "    \n",
        "    # plt.plot(range(len(train_losses)),train_losses,marker='o',label='train')\n",
        "    # plt.plot(range(len(val_losses)),val_losses,marker='o',label = 'validation')\n",
        "    # plt.legend()\n",
        "    # plt.title(machine_type, fontsize=22)\n",
        "    # plt.xlabel(r'Time Step t',fontsize=22)\n",
        "    # plt.ylabel(r'Machine Loss',fontsize=20)\n",
        "    # plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Lns6Rr0BAw"
      },
      "source": [
        "# train_full(data_path,'full')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tpo0MYh0BAw"
      },
      "source": [
        "# def train_g_full(data_path):\n",
        "#     machine_type = 'full'\n",
        "#     print('started training g Using machine model: ',machine_type)\n",
        "#     data = load_data(data_path)\n",
        "#     X = torch.from_numpy(data['X']).float()\n",
        "#     Y = torch.from_numpy(data['Y']).long()\n",
        "#     hloss = data['hloss']\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         mnet = CNN(vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "#                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "#         mnet.load_state_dict(torch.load(model_dir + 'm_full'))\n",
        "\n",
        "#         mnet.to(device)\n",
        "#         mnet.eval()\n",
        "    \n",
        "    \n",
        "#     val_X = torch.from_numpy(data['val']['X']).float()\n",
        "#     val_Y = torch.from_numpy(data['val']['Y']).long()\n",
        "#     val_hloss = data['val']['hloss']\n",
        "    \n",
        "#     batch_size = 128\n",
        "#     num_batches = int(X.shape[0] / batch_size)\n",
        "#     val_num_batches = int(val_X.shape[0] / batch_size)\n",
        "    \n",
        "#     num_epochs = 30\n",
        "    \n",
        "#     gnet = CNN(vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "#                output_dim=2, dropout=0.5).to(device)\n",
        "#     gnet.to(device)\n",
        "#     g_optimizer = torch.optim.Adam(gnet.parameters(),lr=0.001)\n",
        "#     loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "    \n",
        "\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     best_val_loss = 1000\n",
        "#     max_patience = 8\n",
        "#     patience = 0\n",
        "#     eps = 1e-3\n",
        "    \n",
        "#     for epoch in range(num_epochs):\n",
        "#         gprediction = []\n",
        "#         print('----- epoch:',epoch, '-----')\n",
        "#         g_train_loss = 0\n",
        "#         for i in range(num_batches):\n",
        "#             X_batch = X[i * batch_size: (i + 1) * batch_size].to(device)\n",
        "#             Y_batch = Y[i * batch_size: (i + 1) * batch_size].to(device)\n",
        "#             hloss_batch = hloss[i * batch_size: (i + 1) * batch_size].to(device)\n",
        "#             with torch.no_grad():\n",
        "#                 machine_loss_batch = loss_func(mnet(X_batch),Y_batch)\n",
        "#                 machine_indices = find_machine_samples(machine_loss_batch, hloss_batch, constraint=1.0)\n",
        "#             g_labels = torch.tensor([0 if j in machine_indices else 1 for j in range(hloss_batch.shape[0])]).to(device)\n",
        "#             g_optimizer.zero_grad()\n",
        "#             gpred = gnet(X_batch)\n",
        "#             gprediction.extend(gpred[:,1])\n",
        "#             g_loss = loss_func(gpred,g_labels)\n",
        "#             g_loss.sum().backward()\n",
        "#             g_optimizer.step()\n",
        "#             g_train_loss += float(g_loss.mean())\n",
        "\n",
        "            \n",
        "#         train_losses.append(g_train_loss/num_batches)\n",
        "#         print('g_loss:',g_train_loss/num_batches) \n",
        "        \n",
        "#         with torch.no_grad():\n",
        "#             val_gloss = 0\n",
        "#             for i in range(val_num_batches):\n",
        "#                 val_X_batch = val_X[i * batch_size: (i + 1) * batch_size].to(device)\n",
        "#                 val_Y_batch = val_Y[i * batch_size: (i + 1) * batch_size].to(device)\n",
        "#                 val_hloss_batch = val_hloss[i * batch_size: (i + 1) * batch_size].to(device)\n",
        "#                 val_machine_loss = loss_func(mnet(val_X_batch),val_Y_batch)\n",
        "#                 val_machine_indices = find_machine_samples(val_machine_loss,val_hloss_batch,constraint=1.0)\n",
        "#                 val_glabels = torch.tensor([0 if j in val_machine_indices else 1 for j in range(val_X_batch.shape[0])]).to(device)\n",
        "#                 val_loss = loss_func(gnet(val_X_batch),val_glabels)\n",
        "#                 val_gloss += float(val_loss.mean())\n",
        "\n",
        "                \n",
        "#             val_gloss /= val_num_batches\n",
        "#             val_losses.append(val_gloss)\n",
        "#             print('val_g_loss:',float(val_gloss))\n",
        "\n",
        "#             if val_gloss + eps < best_val_loss:\n",
        "#                 torch.save(gnet.state_dict(), model_dir + 'g_' + machine_type)\n",
        "#                 best_val_loss = val_gloss\n",
        "#                 print('updated the model')\n",
        "#                 patience = 0\n",
        "#             else:\n",
        "#                 patience += 1\n",
        "\n",
        "#         if patience > max_patience:\n",
        "#             print('no progress for 10 epochs... stopping training')\n",
        "#             break\n",
        "                \n",
        "#         print('\\n')\n",
        "        \n",
        "#     del gnet\n",
        "#     del mnet\n",
        "        \n",
        "#     plt.plot(range(len(train_losses)),train_losses,marker = 'o',label='train')\n",
        "#     plt.plot(range(len(val_losses)),val_losses,marker = 'o',label='validation')\n",
        "#     plt.title('train and validation curve of g' ,fontsize=22)\n",
        "#     plt.xlabel('Epoch',fontsize=22)\n",
        "#     plt.ylabel(r'g Loss',fontsize=20)\n",
        "#     plt.legend()\n",
        "#     plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlBsHafw0BAw",
        "scrolled": true
      },
      "source": [
        "# train_g_full(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjHYlXUF0BAx"
      },
      "source": [
        "# def get_test_assignments_full(constraints):\n",
        "#     with torch.no_grad():\n",
        "#         machine_type = 'full'\n",
        "#         threshold_map = {0.0:0.0,0.2:0.30,0.4:0.30,0.6:0.30,0.8:0.21,1.0:0.37}\n",
        "#         loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "#         data = load_data(data_path)\n",
        "#         test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "#         test_Y = data['test']['Y']\n",
        "#         hlabel = data['test']['hpred']\n",
        "#         hcrossloss = data['test']['hloss']\n",
        "\n",
        "#         mnet = CNN(vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "#                output_dim=3, dropout=0.5).to(device)\n",
        "#         mnet.to(device)\n",
        "#         mnet.load_state_dict(torch.load(model_dir + 'm_full'))\n",
        "#         mnet.eval()\n",
        "#         mscores = mnet(test_X)\n",
        "#         mcrossloss=loss_func(mscores,torch.tensor(test_Y).to(device))\n",
        "#         mlabel = torch.argmax(mscores,dim=1).cpu().data.numpy()\n",
        "\n",
        "#         gnet = CNN(vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "#                output_dim=2, dropout=0.5).to(device)\n",
        "#         gnet.to(device)\n",
        "#         gnet.load_state_dict(torch.load(model_dir + 'g_full'))\n",
        "#         gnet.eval()\n",
        "#         gprediction = torch.exp(gnet(test_X).detach()[:,1])\n",
        "        \n",
        "        \n",
        "#         argsorted_g = torch.argsort(gprediction)\n",
        "        \n",
        "\n",
        "#         losses = []\n",
        "#         for constraint in constraints:\n",
        "#             num_machine = int((1.0 - constraint) * test_X.shape[0])\n",
        "#             loss = np.zeros(test_X.shape[0])\n",
        "\n",
        "#             human_candidates = torch.argsort(gprediction)[num_machine:]\n",
        "#             to_machine = [i for i in range(mlabel.shape[0]) if i not in human_candidates or gprediction[i]<threshold_map[constraint]]\n",
        "#             to_human = np.array([i for i in range(test_X.shape[0]) if i not in to_machine])\n",
        "            \n",
        "#             mloss = np.not_equal(mlabel,test_Y)\n",
        "#             hloss = np.not_equal(hlabel,test_Y)\n",
        "#             print(len(to_machine),len(to_human))\n",
        "#             if len(to_machine)!=0:\n",
        "#                 loss[to_machine] = mloss[to_machine]\n",
        "#                 print('mean of machine error:' ,np.mean(loss[to_machine]))\n",
        "#             if to_human.shape[0]!=0:\n",
        "#                 loss[to_human] = hloss[to_human]\n",
        "#                 print('mean of human error:' ,np.mean(loss[to_human]))\n",
        "                \n",
        "#             losses.append(np.mean(loss))\n",
        "                \n",
        "\n",
        "#     plt.plot(constraints,losses,marker='o')\n",
        "#     plt.title(r'Full Automaion Triage',fontsize=22)\n",
        "#     plt.ylabel(r'misclassification error',fontsize=22)\n",
        "#     plt.xlabel(r'b',fontsize=22)\n",
        "#     plt.show()\n",
        "    \n",
        "#     if machine_type not in data:\n",
        "#         data[machine_type] = {}\n",
        "        \n",
        "#     data[machine_type]['agg_loss'] = losses\n",
        "#     save_data(data,data_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn2POizJ0BAx"
      },
      "source": [
        "# get_test_assignments_full(constraints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlFx9KFC0BAx"
      },
      "source": [
        "## [Score-based Triage](https://arxiv.org/abs/1903.12220)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl-r3PjN0BAx"
      },
      "source": [
        "for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "  if seed != '':\n",
        "    set_seed(seed)\n",
        "  train_full(seed, data_path,'score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_ohVKFv0BAx"
      },
      "source": [
        "def get_assignments_score(data_path,constraints):\n",
        "    machine_type = 'score'\n",
        "    l = []\n",
        "    for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "      if seed != '':\n",
        "        mct = machine_type + '_seed_' + str(seed)\n",
        "      else:\n",
        "        mct = machine_type\n",
        "      loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "      data = load_data(data_path)\n",
        "      test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "      test_Y = data['test']['Y']\n",
        "      hlabel = data['test']['hpred']\n",
        "      \n",
        "      mnet = CNN(vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "      mnet.load_state_dict(torch.load(model_dir + 'm_' + mct))\n",
        "      mnet.eval()\n",
        "      mlabel = torch.argmax(mnet(test_X),dim=1).cpu().data.numpy()\n",
        "      loss = np.zeros(test_X.shape[0])\n",
        "      mconf,_ = torch.max(mnet(test_X),axis = 1)\n",
        "      \n",
        "      losses = []\n",
        "      for constraint in constraints:\n",
        "          num_machine = int((1.0-constraint) * test_X.shape[0])\n",
        "          to_machine = torch.argsort(mconf,descending = True)[:num_machine].cpu().data.numpy()\n",
        "          to_human = np.array([i for i in range(test_X.shape[0]) if i not in to_machine])\n",
        "\n",
        "          mloss = np.not_equal(mlabel,test_Y)\n",
        "          hloss = np.not_equal(hlabel,test_Y)\n",
        "\n",
        "          loss[to_machine] = mloss[to_machine]\n",
        "          loss[to_human] =  hloss[to_human]\n",
        "\n",
        "          losses.append(np.mean(loss))\n",
        "      l.append(losses)\n",
        "        \n",
        "    if machine_type not in data.keys():\n",
        "        data[machine_type] = {}\n",
        "\n",
        "    l = np.array(l)\n",
        "    print('l shape: ', l.shape)\n",
        "    data[machine_type]['agg_loss'] = l\n",
        "    save_data(data,data_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HArx6ZYt0BAx"
      },
      "source": [
        "get_assignments_score(data_path,constraints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8gO__a70BAx"
      },
      "source": [
        "## [Surrogate-based Triage](https://arxiv.org/abs/2006.01862)\n",
        "The code is taken from [here](https://github.com/clinicalml/learn-to-defer) where the authors provide implementation of their method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx-kHk2i0BAx"
      },
      "source": [
        "def metrics_print(net, val_X, val_Y, val_human_is_correct):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    correct_sys = 0\n",
        "    exp = 0\n",
        "    exp_total = 0\n",
        "    total = 0\n",
        "    real_total = 0\n",
        "    alone_correct = 0\n",
        "    batch_size = 64\n",
        "    num_batches = int(val_X.shape[0] / batch_size)\n",
        "    n_classes = 3\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_batches):\n",
        "            X_batch = val_X[i * batch_size:(i + 1) * batch_size]\n",
        "            Y_batch = val_Y[i * batch_size:(i + 1) * batch_size]\n",
        "            val_human_is_correct_batch = val_human_is_correct[i * batch_size:(i + 1) * batch_size]\n",
        "            outputs = net(X_batch)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            batch_size = outputs.size()[0]            # batch_size\n",
        "            for i in range(0,batch_size):\n",
        "                r = (predicted[i].item() == 3)\n",
        "                prediction = predicted[i]\n",
        "                if predicted[i] == n_classes:\n",
        "                    max_idx = 0\n",
        "\t\t\t\t\t          # get second max\n",
        "                    for j in range(0, n_classes):\n",
        "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
        "                          max_idx = j\n",
        "                    prediction = max_idx\n",
        "                else:\n",
        "                  prediction = predicted[i]\n",
        "                alone_correct += (prediction == Y_batch[i]).item()\n",
        "                if r==0:\n",
        "                    total += 1\n",
        "                    correct += (predicted[i] == Y_batch[i]).item()\n",
        "                    correct_sys += (predicted[i] == Y_batch[i]).item()\n",
        "                if r==1:\n",
        "                    exp += val_human_is_correct_batch[i].item()\n",
        "                    correct_sys += val_human_is_correct_batch[i].item()\n",
        "                    exp_total+=1\n",
        "                real_total += 1\n",
        "    cov = str(total) + str(\" out of\") + str(real_total)\n",
        "    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001), \"alone classifier\": 100*alone_correct/real_total }\n",
        "    print(to_print)\n",
        "    return [100*total/real_total,  100*correct_sys/real_total, 100* exp/(exp_total+0.0002),100*correct/(total+0.0001) ]\n",
        "\n",
        "\n",
        "def surrogate_train(seed, data_path, alpha=1.0):\n",
        "\n",
        "    def loss_func(outputs, m, labels, m2, n_classes):\n",
        "        '''\n",
        "        The L_{CE} loss implementation for hatespeech, identical to CIFAR implementation\n",
        "        ----\n",
        "        outputs: network outputs\n",
        "        m: cost of deferring to expert cost of classifier predicting (alpha* I_{m\\neq y} + I_{m =y})\n",
        "        labels: target\n",
        "        m2:  cost of classifier predicting (alpha* I_{m\\neq y} + I_{m =y})\n",
        "        n_classes: number of classes\n",
        "        '''\n",
        "        batch_size = outputs.shape[0] # batch_size\n",
        "        rc = [n_classes] * batch_size\n",
        "\n",
        "        rc = torch.tensor(rc)\n",
        "\n",
        "        outputs = -m * torch.log2(outputs[range(batch_size), rc]) - m2 * torch.log2(outputs[range(batch_size), labels])  # pick the values corresponding to the labels\n",
        "        return torch.sum(outputs)/batch_size\n",
        "    \n",
        "    print('-----training machine model : surrogate')\n",
        "    machine_type = 'surrogate'\n",
        "    if seed != '':\n",
        "      mct = machine_type + '_seed_' + str(seed)\n",
        "    else:\n",
        "      mct = machine_type\n",
        "    data = load_data(data_path)\n",
        "    X = torch.from_numpy(data['X']).float().to(device)\n",
        "    Y = torch.from_numpy(data['Y']).to(device).long()\n",
        "    human_is_correct = torch.from_numpy(np.array([1 if data['hpred'][i]==data['Y'][i] else 0\n",
        "                                                  for i in range(X.shape[0])])).to(device)\n",
        "\n",
        "    print(torch.mean(human_is_correct.float()))\n",
        "    #alpha = 1.0\n",
        "    m = (human_is_correct) * 1.0\n",
        "    m2 = [0.0] * X.shape[0]\n",
        "    for i, j in enumerate(m):\n",
        "      if j == 1.0:\n",
        "        m2[i] = alpha\n",
        "      if j == 0.0:\n",
        "        m2[i] = 1.0\n",
        "    m2 = torch.tensor(m2)\n",
        "    m2 = m2.to(device)\n",
        "\n",
        "\n",
        "    val_X = torch.from_numpy(data['val']['X']).float().to(device)\n",
        "    val_Y = torch.from_numpy(data['val']['Y']).to(device).long()\n",
        "    val_human_is_correct = torch.from_numpy(np.array([1 if data['val']['hpred'][i] == data['val']['Y'][i] else 0\n",
        "                                                      for i in range(val_X.shape[0])])).to(device)\n",
        "    val_m = (val_human_is_correct) * 1.0\n",
        "    val_m2 = [0.0] * val_X.shape[0]\n",
        "    for i, j in enumerate(val_m):\n",
        "      if j == 1.0:\n",
        "        val_m2[i] = alpha\n",
        "      if j == 0.0:\n",
        "        val_m2[i] = 1.0\n",
        "    val_m2 = torch.tensor(val_m2)\n",
        "    val_m2 = val_m2.to(device)\n",
        "    \n",
        "    batch_size = 64\n",
        "    num_epochs = 100\n",
        "\n",
        "    num_batches = int(X.shape[0] / batch_size)\n",
        "    \n",
        "    N_FILTERS = 300  # hyperparameterr\n",
        "    FILTER_SIZES = [3, 4, 5]\n",
        "    DROPOUT = 0.5\n",
        "    output_dim = 3\n",
        "    mnet = CNN_rej(embedding_dim = 100,vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "               output_dim=3, dropout=0.5).to(device)\n",
        "    print(mnet)\n",
        "    optimizer = torch.optim.Adam(mnet.parameters(),lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_batches * num_epochs)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = 1000\n",
        "    max_patience = 10\n",
        "    patience = 0\n",
        "    eps = 1e-4\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('----- epoch:',epoch, '-----')\n",
        "        epoch_loss = 0\n",
        "        val_epoch_loss = 0\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X[i * batch_size:(i + 1) * batch_size]\n",
        "            Y_batch = Y[i * batch_size:(i + 1) * batch_size]\n",
        "            m_batch = m[i * batch_size:(i + 1) * batch_size]\n",
        "            m2_batch = m2[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_func(mnet(X_batch),m_batch,Y_batch,m2_batch,output_dim)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += float(loss)\n",
        "\n",
        "\n",
        "        print('train loss: ',epoch_loss/ num_batches)\n",
        "        train_losses.append(epoch_loss / num_batches)\n",
        "        _ = metrics_print(mnet, val_X, val_Y, val_human_is_correct)\n",
        "        with torch.no_grad():\n",
        "            m_val_prob = mnet(val_X)\n",
        "            val_loss = loss_func(m_val_prob, val_m, val_Y, val_m2, output_dim)\n",
        "            val_losses.append(float(val_loss))\n",
        "            print('validation loss: ', float(val_loss))\n",
        "            \n",
        "            if val_loss < best_val_loss:\n",
        "                torch.save(mnet.state_dict(), model_dir + 'm_' + mct)\n",
        "                best_val_loss = val_loss\n",
        "                print('updated the model')\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "            \n",
        "            if patience > max_patience:\n",
        "                print('no progress for 10 epochs... stopping training')\n",
        "                break\n",
        "    \n",
        "        print('\\n')\n",
        "\n",
        "    plt.plot(range(len(train_losses)),train_losses,marker='o', label = 'train')\n",
        "    plt.plot(range(len(val_losses)),val_losses,marker='o', label = 'validation')\n",
        "    plt.legend()\n",
        "    plt.title(machine_type,fontsize=22)\n",
        "    plt.xlabel(r'Time Step t',fontsize=22)\n",
        "    plt.ylabel(r'Machine Loss',fontsize=20)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vWkUJ10BAy",
        "scrolled": true
      },
      "source": [
        "for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "    print(\"training with seed {}\".format(seed))\n",
        "    if seed != '':\n",
        "      set_seed(seed)\n",
        "    surrogate_train(seed, data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xkiZiEuxpLI"
      },
      "source": [
        "# data = load_data(data_path)\n",
        "# test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "# test_Y = data['test']['Y']\n",
        "# hlabel = data['test']['hpred']\n",
        "# test_human_is_correct = torch.from_numpy(np.array([1 if data['test']['hpred'][i] == data['test']['Y'][i] else 0\n",
        "#                                                       for i in range(test_X.shape[0])])).to(device)\n",
        "# mnet = CNN_rej(embedding_dim=100,vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "#                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "# mnet.load_state_dict(torch.load(model_dir + 'm_surrogate')) # change accordingly for different seeds\n",
        "# mnet.eval()\n",
        "\n",
        "# metrics_print(mnet, test_X, test_Y, test_human_is_correct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcajA5Os0BAy"
      },
      "source": [
        "def get_test_assignments_surrogate(data_path,constraints):\n",
        "    machine_type = 'surrogate'\n",
        "    l = []\n",
        "    for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "      if seed != '':\n",
        "        mct = machine_type + '_seed_' + str(seed)\n",
        "      else:\n",
        "        mct = machine_type\n",
        "      loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "      data = load_data(data_path)\n",
        "      test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "      test_Y = data['test']['Y']\n",
        "      hlabel = data['test']['hpred']\n",
        "      \n",
        "      mnet = CNN_rej(embedding_dim=100,vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "      mnet.load_state_dict(torch.load(model_dir + 'm_' + mct))\n",
        "      mnet.eval()\n",
        "      mscores = mnet(test_X)\n",
        "      temp = torch.sum(mscores[:,:3], dim=1, keepdim=True)\n",
        "      mscores = torch.divide(mscores, temp)\n",
        "      assert(mscores.shape[1]==4)\n",
        "      mlabel = torch.argmax(mscores[:,:-1],dim=1).cpu().data.numpy()\n",
        "      assert (mlabel<4).all()\n",
        "      loss = np.zeros(test_X.shape[0])\n",
        "      last_class_prob = mscores[:,-1]\n",
        "      highest_prob,_ = torch.max(mscores[:,:-1],dim=1)\n",
        "      diff =  last_class_prob - highest_prob\n",
        "      \n",
        "      hloss = np.not_equal(hlabel,test_Y)\n",
        "\n",
        "      losses = []\n",
        "      for constraint in constraints:\n",
        "          num_machine = int((1.0-constraint) * test_X.shape[0])\n",
        "          to_machine = torch.argsort(diff)[:num_machine].cpu().data.numpy()\n",
        "          to_human = np.array([i for i in range(test_X.shape[0]) if i not in to_machine])\n",
        "\n",
        "          mloss = np.not_equal(mlabel,test_Y)\n",
        "          hloss = np.not_equal(hlabel,test_Y)\n",
        "\n",
        "          loss[to_machine] = mloss[to_machine]\n",
        "          loss[to_human] =  hloss[to_human]\n",
        "\n",
        "          losses.append(np.mean(loss))\n",
        "      \n",
        "      l.append(losses)\n",
        "\n",
        "    if machine_type not in data.keys():\n",
        "        data[machine_type] = {}\n",
        "    l = np.array(l)\n",
        "    data[machine_type]['agg_loss'] = l\n",
        "    save_data(data,data_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElCw0ZqS0BA1"
      },
      "source": [
        "get_test_assignments_surrogate(data_path,constraints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXeah0FNMAVv"
      },
      "source": [
        "### OvA Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKaPGbC4L-zK"
      },
      "source": [
        "def metrics_print_ova(net, val_X, val_Y, val_human_is_correct):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    correct_sys = 0\n",
        "    exp = 0\n",
        "    exp_total = 0\n",
        "    total = 0\n",
        "    real_total = 0\n",
        "    alone_correct = 0\n",
        "    batch_size = 64\n",
        "    num_batches = int(val_X.shape[0] / batch_size)\n",
        "    n_classes = 3\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_batches):\n",
        "            X_batch = val_X[i * batch_size:(i + 1) * batch_size]\n",
        "            Y_batch = val_Y[i * batch_size:(i + 1) * batch_size]\n",
        "            val_human_is_correct_batch = val_human_is_correct[i * batch_size:(i + 1) * batch_size]\n",
        "            _, outputs = net(X_batch)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            batch_size = outputs.size()[0]            # batch_size\n",
        "            for i in range(0,batch_size):\n",
        "                r = (predicted[i].item() == 3)\n",
        "                prediction = predicted[i]\n",
        "                if predicted[i] == n_classes:\n",
        "                    max_idx = 0\n",
        "                              # get second max\n",
        "                    for j in range(0, n_classes):\n",
        "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
        "                          max_idx = j\n",
        "                    prediction = max_idx\n",
        "                else:\n",
        "                  prediction = predicted[i]\n",
        "                alone_correct += (prediction == Y_batch[i]).item()\n",
        "                if r==0:\n",
        "                    total += 1\n",
        "                    correct += (predicted[i] == Y_batch[i]).item()\n",
        "                    correct_sys += (predicted[i] == Y_batch[i]).item()\n",
        "                if r==1:\n",
        "                    exp += val_human_is_correct_batch[i].item()\n",
        "                    correct_sys += val_human_is_correct_batch[i].item()\n",
        "                    exp_total+=1\n",
        "                real_total += 1\n",
        "    cov = str(total) + str(\" out of\") + str(real_total)\n",
        "    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001), \"alone classifier\": 100*alone_correct/real_total }\n",
        "    print(to_print)\n",
        "    return [100*total/real_total,  100*correct_sys/real_total, 100* exp/(exp_total+0.0002),100*correct/(total+0.0001) ]\n",
        "\n",
        "def loss_func_ova(outputs, labels, m, m2, n_classes):\n",
        "    batch_size = outputs.size()[0]\n",
        "    l1 = LogisticLoss(outputs[range(batch_size), labels], 1)\n",
        "    l2 = torch.sum(LogisticLoss(outputs[:,:n_classes], -1), dim=1) - LogisticLoss(outputs[range(batch_size),labels],-1)\n",
        "    l3 = LogisticLoss(outputs[range(batch_size), n_classes], -1)\n",
        "    l4 = LogisticLoss(outputs[range(batch_size), n_classes], 1)\n",
        "\n",
        "    l5 = m*(l4 - l3)\n",
        "\n",
        "    l = m2*(l1 + l2) + l3 + l5\n",
        "    return torch.mean(l)\n",
        "\n",
        "\n",
        "def LogisticLoss(outputs, y):\n",
        "    outputs[torch.where(outputs==0.0)] = (-1*y)*(-1*np.inf)\n",
        "    l = torch.log2(1 + torch.exp((-1*y)*outputs))\n",
        "    return l\n",
        "\n",
        "\n",
        "\n",
        "def surrogate_train_ova(seed, data_path, alpha=1.0):    \n",
        "    print('-----training machine model : surrogate')\n",
        "    machine_type = 'surrogate_ova'\n",
        "    if seed != '':\n",
        "      mct = machine_type + '_seed_' + str(seed)\n",
        "    else:\n",
        "      mct = machine_type\n",
        "    data = load_data(data_path)\n",
        "    X = torch.from_numpy(data['X']).float().to(device)\n",
        "    Y = torch.from_numpy(data['Y']).to(device).long()\n",
        "    human_is_correct = torch.from_numpy(np.array([1 if data['hpred'][i]==data['Y'][i] else 0\n",
        "                                                  for i in range(X.shape[0])])).to(device)\n",
        "\n",
        "    print(torch.mean(human_is_correct.float()))\n",
        "    alpha = 1.0\n",
        "    m = (human_is_correct) * 1.0\n",
        "    m2 = [0.0] * X.shape[0]\n",
        "    for i, j in enumerate(m):\n",
        "      if j == 1.0:\n",
        "          m2[i] = alpha\n",
        "      if j == 0.0:\n",
        "          m2[i] = 1.0\n",
        "    m2 = torch.tensor(m2).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    val_X = torch.from_numpy(data['val']['X']).float().to(device)\n",
        "    val_Y = torch.from_numpy(data['val']['Y']).to(device).long()\n",
        "    val_human_is_correct = torch.from_numpy(np.array([1 if data['val']['hpred'][i] == data['val']['Y'][i] else 0\n",
        "                                                      for i in range(val_X.shape[0])])).to(device)\n",
        "    val_m = (val_human_is_correct) * 1.0\n",
        "    val_m2 = [0.0] * val_X.shape[0]\n",
        "    for i, j in enumerate(val_m):\n",
        "      if j == 1.0:\n",
        "        val_m2 = alpha\n",
        "      if j == 0.0:\n",
        "        val_m2 = 1.0\n",
        "    val_m2 = torch.tensor(val_m2).to(device)\n",
        "    \n",
        "    batch_size = 64\n",
        "    num_epochs = 200\n",
        "\n",
        "    num_batches = int(X.shape[0] / batch_size)\n",
        "    \n",
        "    N_FILTERS = 300  # hyperparameterr\n",
        "    FILTER_SIZES = [3, 4, 5]\n",
        "    DROPOUT = 0.5\n",
        "    output_dim = 3\n",
        "    mnet = CNN_rej_ova(embedding_dim = 100,vocab_size=X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "               output_dim=3, dropout=0.5).to(device)\n",
        "    print(mnet)\n",
        "    optimizer = torch.optim.Adam(mnet.parameters(),lr=0.005)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_batches * num_epochs)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = 1000\n",
        "    max_patience = 10\n",
        "    patience = 0\n",
        "    eps = 1e-4\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('----- epoch:',epoch, '-----')\n",
        "        epoch_loss = 0\n",
        "        val_epoch_loss = 0\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X[i * batch_size:(i + 1) * batch_size]\n",
        "            Y_batch = Y[i * batch_size:(i + 1) * batch_size]\n",
        "            m_batch = m[i * batch_size:(i + 1) * batch_size]\n",
        "            m2_batch = m2[i * batch_size:(i + 1) * batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_func_ova(mnet(X_batch)[0],Y_batch, m_batch, m2_batch, output_dim)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += float(loss)\n",
        "\n",
        "\n",
        "        print('train loss: ',epoch_loss/ num_batches)\n",
        "        train_losses.append(epoch_loss / num_batches)\n",
        "        _ = metrics_print_ova(mnet, val_X, val_Y, val_human_is_correct)\n",
        "        with torch.no_grad():\n",
        "            m_val_prob,_ = mnet(val_X)\n",
        "            val_loss = loss_func_ova(m_val_prob, val_Y, val_m, val_m2, output_dim)\n",
        "            val_losses.append(float(val_loss))\n",
        "            print('validation loss: ', float(val_loss))\n",
        "            \n",
        "            if val_loss < best_val_loss:\n",
        "                torch.save(mnet.state_dict(), model_dir + 'm_' + mct)\n",
        "                best_val_loss = val_loss\n",
        "                print('updated the model')\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "            \n",
        "            if patience > max_patience:\n",
        "                print('no progress for 10 epochs... stopping training')\n",
        "                break\n",
        "    \n",
        "        print('\\n')\n",
        "               \n",
        "    plt.plot(range(len(train_losses)),train_losses,marker='o', label = 'train')\n",
        "    plt.plot(range(len(val_losses)),val_losses,marker='o', label = 'validation')\n",
        "    plt.legend()\n",
        "    plt.title(machine_type,fontsize=22)\n",
        "    plt.xlabel(r'Time Step t',fontsize=22)\n",
        "    plt.ylabel(r'Machine Loss',fontsize=20)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T6pbMPnMRii"
      },
      "source": [
        "for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "    print(\"training with seed {}\".format(seed))\n",
        "    if seed != '':\n",
        "      set_seed(seed)\n",
        "    surrogate_train_ova(seed, data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3JntDRGOeaX"
      },
      "source": [
        "# data = load_data(data_path)\n",
        "# test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "# test_Y = data['test']['Y']\n",
        "# hlabel = data['test']['hpred']\n",
        "# test_human_is_correct = torch.from_numpy(np.array([1 if data['test']['hpred'][i] == data['test']['Y'][i] else 0\n",
        "#                                                       for i in range(test_X.shape[0])])).to(device)\n",
        "# mnet = CNN_rej_ova(embedding_dim=100,vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "#                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "# mnet.load_state_dict(torch.load(model_dir + 'm_surrogate_ova_new')) # change accordingly for different seeds\n",
        "# mnet.eval()\n",
        "\n",
        "# metrics_print_ova(mnet, test_X, test_Y, test_human_is_correct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdVd9cqBOomH"
      },
      "source": [
        "def get_test_assignments_surrogate_ova(data_path, constraints):\n",
        "    machine_type = 'surrogate_ova'\n",
        "    l = []\n",
        "    for seed in ['', 948,  625,  436,  791, 1750]:\n",
        "      if seed != '':\n",
        "        mct = machine_type + '_seed_' + str(seed)\n",
        "      else:\n",
        "        mct = machine_type\n",
        "      loss_func = torch.nn.NLLLoss(reduction='none')\n",
        "      data = load_data(data_path)\n",
        "      test_X = torch.from_numpy(data['test']['X']).float().to(device)\n",
        "      test_Y = data['test']['Y']\n",
        "      hlabel = data['test']['hpred']\n",
        "      \n",
        "      mnet = CNN_rej_ova(embedding_dim=100,vocab_size=test_X.shape[1], n_filters=300, filter_sizes=[3,4,5],\n",
        "                output_dim=3, dropout=0.5).to(device)\n",
        "\n",
        "      mnet.load_state_dict(torch.load(model_dir + 'm_' + mct))\n",
        "      mnet.eval()\n",
        "      _, mscores = mnet(test_X)\n",
        "      assert(mscores.shape[1]==4)\n",
        "      mlabel = torch.argmax(mscores[:,:-1],dim=1).cpu().data.numpy()\n",
        "      assert (mlabel<4).all()\n",
        "      loss = np.zeros(test_X.shape[0])\n",
        "      last_class_prob = mscores[:,-1]\n",
        "      highest_prob,_ = torch.max(mscores[:,:-1],dim=1)\n",
        "      diff =  last_class_prob - highest_prob\n",
        "      \n",
        "      hloss = np.not_equal(hlabel,test_Y)\n",
        "\n",
        "      to_defer = torch.where(diff > 0)[0]\n",
        "\n",
        "      ids,_ = torch.sort(to_defer)\n",
        "\n",
        "      losses = []\n",
        "      for constraint in constraints:\n",
        "          num_machine = int((1.0-constraint) * test_X.shape[0])\n",
        "          human = torch.argsort(diff)[num_machine:].cpu().data.numpy()\n",
        "\n",
        "          if len(human) >= len(ids):\n",
        "              human_candidates = ids\n",
        "          else:\n",
        "              human_candidates = human\n",
        "\n",
        "          to_machine = [i for i in range(mlabel.shape[0]) if i not in human_candidates]\n",
        "      \n",
        "          to_human = np.array([i for i in range(mlabel.shape[0]) if i not in to_machine])\n",
        "\n",
        "          mloss = np.not_equal(mlabel,test_Y)\n",
        "          hloss = np.not_equal(hlabel,test_Y)\n",
        "\n",
        "          loss[to_machine] = mloss[to_machine]\n",
        "          loss[to_human] =  hloss[to_human]\n",
        "\n",
        "          losses.append(np.mean(loss))\n",
        "      l.append(losses)\n",
        "    \n",
        "    if machine_type not in data.keys():\n",
        "        data[machine_type] = {}\n",
        "    l = np.array(l)\n",
        "    data[machine_type]['agg_loss'] = l\n",
        "    save_data(data,data_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOJL36cJOvBG"
      },
      "source": [
        "get_test_assignments_surrogate_ova(data_path, constraints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting\n"
      ],
      "metadata": {
        "id": "tcFLUGqJEQYS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5bm3Yos0BA1"
      },
      "source": [
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rc('font', weight='bold')\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "def plot_misclassification_loss(data_path,machine_types):\n",
        "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "    data = load_data(data_path)\n",
        "    colors = {'surrogate': 'blue', 'surrogate_ova' : 'green',  'confidence': 'purple', 'score' : 'firebrick', 'differentiable': 'orange'}\n",
        "    for machine_type in machine_types:\n",
        "        agg_loss = data[machine_type]['agg_loss']\n",
        "        print(agg_loss.shape)\n",
        "        if machine_type == \"Differentiable\":\n",
        "          machine_type = \"differentiable\"\n",
        "        mean = np.mean(agg_loss, axis=0)\n",
        "        standard_error = stats.sem(agg_loss, axis=0)\n",
        "        ax.plot(constraints,mean,marker='o',label=machine_type, color=colors[machine_type])\n",
        "        ax.fill_between(constraints, mean-standard_error, mean+standard_error, alpha=0.3, color=colors[machine_type])\n",
        "    ax.set_xticks(constraints)\n",
        "    ax.set_ylabel(r'$misclassification$ ' + '$error$',fontsize=15)\n",
        "    ax.set_xlabel(r'$budget$',fontsize=15)\n",
        "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "PQH6bASGdGRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qIZdG9y0BA1"
      },
      "source": [
        "machine_types =  ['surrogate', 'surrogate_ova', 'score','confidence','Differentiable']\n",
        "fig = plot_misclassification_loss(data_path,machine_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brvFlAREUSCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFken9mt0BA1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}